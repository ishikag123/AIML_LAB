{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfFIjcpHMnZXKFr1+KQk6r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#  Spam Filter with Naive Bayes\n",
        "#========================================\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from IPython.display import display\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# Activate the necessary magics\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format ='retina'\n",
        "# Setting display options\n",
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth',-1)\n",
        "# Read in data\n",
        "spam_collection = pd.read_csv('SMSSpamCollection.csv', sep='\\t', header=None,names=['Label', 'SMS'])\n",
        "print(spam_collection.shape)\n",
        "spam_collection.head()\n",
        "spam_collection.info()\n",
        "spam_collection['Label'].value_counts(normalize = True)\n",
        "# Original Data (13.4% of the messages are spam, while the rest are ham)\n",
        "fig1, ax1 = plt.subplots(figsize=(5,5))\n",
        "labels = ['Spam', 'Ham']\n",
        "sizes = [len(spam_collection[spam_collection['Label'] == 'spam']),\n",
        "len(spam_collection[spam_collection['Label'] == 'ham'])]\n",
        "explode = (0, 0.1)  # only \"explode\" the 2nd slice\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False,startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "ax1.set_title(\"Original Data Set\", fontsize=14)\n",
        "    \n",
        "plt.show()\n",
        "# Random data show\n",
        "texts = spam_collection['SMS'].sum()\n",
        "wc = WordCloud(max_words=1000,contour_width=3, contour_color='red')\n",
        "wc.generate(texts)\n",
        "plt.figure(figsize=[15,7])\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.rcParams['savefig.dpi'] = 1100\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "# Splitting Data\n",
        "#A split‐up of 80% and 20%, respectively\n",
        "# Randomize the entire data set\n",
        "randomized_collection = spam_collection.sample(frac=1, random_state=3)\n",
        "# Calculate index for split\n",
        "training_test_index = round(len(randomized_collection) * 0.8)\n",
        "# Training/Test split\n",
        "training_set = randomized_collection[:training_test_index].reset_index(drop=True)\n",
        "test_set = randomized_collection[training_test_index:].reset_index(drop=True)\n",
        "print('Training Data:')\n",
        "print(training_set.shape)\n",
        "print('Testing Data:')\n",
        "print(test_set.shape)\n",
        "print('Training set:\\n', training_set['Label'].value_counts(normalize =True),'\\n\\nTest set:')\n",
        "test_set['Label'].value_counts(normalize = True)\n",
        "# Training Set\n",
        "fig2, ax2 = plt.subplots(figsize=(5,5))\n",
        "labels = ['Spam', 'Ham']\n",
        "sizes = [len(training_set[training_set['Label'] == 'spam']),\n",
        "len(training_set[training_set['Label'] == 'ham'])]\n",
        "explode = (0, 0.1)  # only \"explode\" the 2nd slice\n",
        "ax2.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False,startangle=90)\n",
        "ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "ax2.set_title(\"Training Set\", fontsize=14)\n",
        "    \n",
        "plt.show()\n",
        "# Test Set\n",
        "fig3, ax3 = plt.subplots(figsize=(5,5))\n",
        "labels = ['Spam', 'Ham']\n",
        "sizes = [len(test_set[test_set['Label'] == 'spam']), len(test_set[test_set['Label']== 'ham'])]\n",
        "explode = (0, 0.1)  # only \"explode\" the 2nd slice\n",
        "ax3.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False,startangle=90)\n",
        "ax3.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "ax3.set_title(\"Test Set\", fontsize=14)\n",
        "    \n",
        "plt.show()\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "# MODEL DEVELOPMENT\n",
        "#...............................\n",
        "# Data Pre‐Processing\n",
        "#*****************************************************\n",
        "# 1. Normalization\n",
        "# Original training set ‐ before processing\n",
        "training_set.head()\n",
        "# Replace addresses (hhtp, email), numbers (plain, phone), money symbols\n",
        "training_set['SMS'] =training_set['SMS'].str.replace(r'\\b[\\w\\‐.]+?@\\w+?\\.\\w{2,4}\\b',' ')\n",
        "training_set['SMS'] =training_set['SMS'].str.replace(r'(http[s]?\\S+)|(\\w+\\.[A‐Za‐z]{2,4}\\S*)',\n",
        "                                          ' ')\n",
        "training_set['SMS'] = training_set['SMS'].str.replace(r'£|\\$', ' ')    \n",
        "training_set['SMS'] =training_set['SMS'].str.replace(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\‐(.]?\\d{3}\\)?[\\s.‐]?\\d{3}[\\s.‐]?\\d{4}\\b',' ')    \n",
        "training_set['SMS'] = training_set['SMS'].str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
        "# Remove punctuation, collapse all whitespace (spaces, line breaks, tabs) into a single space & eliminate any leading/trailing whitespace.\n",
        "training_set['SMS'] = training_set['SMS'].str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "training_set['SMS'] = training_set['SMS'].str.replace(r'\\s+', ' ')\n",
        "training_set['SMS'] = training_set['SMS'].str.replace(r'^\\s+|\\s+?$', '')\n",
        "# Lowercase the entire corpus\n",
        "training_set['SMS'] = training_set['SMS'].str.lower()\n",
        "training_set.head()\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "# Natural Language Tool\n",
        "import nltk  \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "#.............................\n",
        "# removing stopword like verb, article ect.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "training_set['SMS'] = training_set['SMS'].apply(lambda x: ' '.join(\n",
        "    term for term in x.split() if term not in set(stop_words))\n",
        ")\n",
        "training_set.head()\n",
        "# Lemmatization: the process of grouping together different inflected forms of the same word\n",
        "#.........................................................\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "training_set['SMS'] = training_set['SMS'].apply(lambda x: ' '.join(\n",
        "    lemmatizer.lemmatize(term, pos='v') for term in x.split())\n",
        ")\n",
        "training_set.head()\n",
        "#Stemming:reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\".\n",
        "#.....................................................\n",
        "porter = nltk.PorterStemmer()\n",
        "training_set['SMS'] = training_set['SMS'].apply(lambda x: ' '.join(\n",
        "    porter.stem(term) for term in x.split())\n",
        ")\n",
        "training_set.head()\n",
        "# 2. Tokenization:  the process of replacing sensitive data with unique identification symbols that retain all the essential information about the data without compromising its security.\n",
        "#***************************\n",
        "training_set['SMS'] = training_set['SMS'].apply(lambda sms:nltk.word_tokenize(sms))\n",
        "training_set.head()\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "#Feature Extraction\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "corpus = training_set['SMS'].sum()\n",
        "len(corpus)\n",
        "# Transform the list to a set, to remove duplicates\n",
        "temp_set = set(corpus)\n",
        "# Revert to a list\n",
        "vocabulary = list(temp_set)\n",
        "len(vocabulary)\n",
        "# Create the dictionary\n",
        "len_training_set = len(training_set['SMS'])\n",
        "word_counts_per_sms = {unique_word: [0] * len_training_set for unique_word in vocabulary}\n",
        "for index, sms in enumerate(training_set['SMS']):\n",
        "    for word in sms:\n",
        "        word_counts_per_sms[word][index] += 1\n",
        "# Convert to dataframe\n",
        "word_counts = pd.DataFrame(word_counts_per_sms)\n",
        "word_counts.head()\n",
        "word_counts.shape\n",
        "# Concatenate with the original training set\n",
        "training_set_final = pd.concat([training_set, word_counts], axis=1)\n",
        "training_set_final.head()\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "# Calculating Probability\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "# Filter the spam and ham dataframes\n",
        "spam_df = training_set_final[training_set_final['Label'] == 'spam'].copy()\n",
        "ham_df = training_set_final[training_set_final['Label'] == 'ham'].copy()\n",
        "# Calculate P(Spam) and P(Ham)\n",
        "p_spam = spam_df.shape[0] / training_set_final.shape[0]\n",
        "p_ham = ham_df.shape[0] / training_set_final.shape[0]\n",
        "print(p_spam)\n",
        "p_ham\n",
        "# Calculate Nspam, Nham and Nvocabulary\n",
        "spam_words_per_message = spam_df['SMS'].apply(len)\n",
        "n_spam = spam_words_per_message.sum()\n",
        "ham_words_per_message = ham_df['SMS'].apply(len)\n",
        "n_ham = ham_words_per_message.sum()\n",
        "n_vocabulary = len(vocabulary)\n",
        "# Opting for the Laplace smoothing to remove 0 probability problem\n",
        "alpha = 1\n",
        "#‐‐‐‐‐‐‐‐‐‐‐\n",
        "#Calculating Parameters\n",
        "# P(wi|Spam) and P(wi|Ham) depend on the training set, which doesn't change, thus they are constant.\n",
        "# Create two dictionaries that match each unique word with the respective probability value.\n",
        "parameters_spam = {unique_word: 0 for unique_word in vocabulary}\n",
        "parameters_ham = {unique_word: 0 for unique_word in vocabulary}\n",
        "# Iterate over the vocabulary and for each word, calculate P(wi|Spam) and P(wi|Ham)\n",
        "for unique_word in vocabulary:\n",
        "    p_unique_word_spam = (spam_df[unique_word].sum() + alpha) / (n_spam + alpha *n_vocabulary)\n",
        "    p_unique_word_ham = (ham_df[unique_word].sum() + alpha) / (n_ham + alpha *n_vocabulary)\n",
        "    \n",
        "    # Update the calculated propabilities to the dictionaries\n",
        "    parameters_spam[unique_word] = p_unique_word_spam\n",
        "    parameters_ham[unique_word] = p_unique_word_ham\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "#Classifying A New Message\n",
        "#....................................\n",
        "def sms_classify(message):\n",
        "    '''\n",
        "    Takes in as input a new sms (w1, w2, ..., wn),\n",
        "    calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn),\n",
        "    compares them and outcomes whether the message is spam or not.\n",
        "    '''\n",
        "    \n",
        "    # Replace addresses (hhtp, email), numbers (plain, phone), money symbols\n",
        "    message = message.replace(r'\\b[\\w\\‐.]+?@\\w+?\\.\\w{2,4}\\b', ' ')\n",
        "    message = message.replace(r'(http[s]?\\S+)|(\\w+\\.[A‐Za‐z]{2,4}\\S*)', ' ')\n",
        "    message = message.replace(r'£|\\$', ' ')    \n",
        "    message =message.replace(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\‐(.]?\\d{3}\\)?[\\s.‐]?\\d{3}[\\s.‐]?\\d{4}\\b', '')    \n",
        "    message = message.replace(r'\\d+(\\.\\d+)?', ' ')\n",
        "    # Remove punctuation, collapse all whitespace (spaces, line breaks, tabs) into a single space & eliminate any leading/trailing whitespace.\n",
        "    message = message.replace(r'[^\\w\\d\\s]', ' ')\n",
        "    message = message.replace(r'\\s+', ' ')\n",
        "    message = message.replace(r'^\\s+|\\s+?$', '')\n",
        "    # Lowercase the entire corpus\n",
        "    message = message.lower()\n",
        "    # Remove stop words    \n",
        "    terms = []\n",
        "    for term in message.split():\n",
        "      if term not in set(stop_words):\n",
        "            terms.append(term)\n",
        "            message = ' '.join(terms)\n",
        "    # Lemmatization\n",
        "    message = ' '.join(lemmatizer.lemmatize(term, pos='v') for term in message.split())            \n",
        "            \n",
        "    # Stemming\n",
        "    message = ' '.join(porter.stem(term) for term in message.split())  \n",
        "    \n",
        "    # Tokenization\n",
        "    message = message.split()\n",
        "    \n",
        "    p_spam_given_message = p_spam\n",
        "    p_ham_given_message = p_ham\n",
        "    \n",
        "    for word in message:\n",
        "        if word in parameters_spam:\n",
        "            p_spam_given_message *= parameters_spam[word]\n",
        "    \n",
        "        if word in parameters_ham:\n",
        "            p_ham_given_message *= parameters_ham[word]\n",
        "    \n",
        "    print('P(Spam|message):', p_spam_given_message)\n",
        "    print('P(Ham|message):', p_ham_given_message)\n",
        "    if p_ham_given_message > p_spam_given_message:\n",
        "        print('Label: Ham')\n",
        "    elif p_ham_given_message < p_spam_given_message:\n",
        "        print('Label: Spam')\n",
        "    else:\n",
        "        print('Equal probabilities ~ Human action needed!')\n",
        "print(\"Test with message: Hey, Sign up with this promo code and get your card for amazing exchange fees abroad and  £5 to spend anywhere! Promocode: D48KV7BN\")\n",
        "sms_classify('''Hey, Sign up with this promo code and get your card for amazing exchange fees abroad and £5 to spend anywhere! Promocode:D48KV7BN''')\n",
        "print('Test with message: Okey Stan! Seems to be a reasonable amount of money. Ill think of it and let you know ASAP.')\n",
        "sms_classify('''Okey Stan! Seems to be a reasonable amount of money. I'll think of it and let you know ASAP.''')\n",
        "print('Test with any massage')\n",
        "sms_classify(input('input text:'))\n",
        "#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐\n",
        "#Measuring the Spam Filter's Accuracy\n",
        "# We define the classify () function again, this time returning the outcomes\n",
        "def sms_classify_test_set(message):\n",
        "    '''\n",
        "    Takes in as input a new sms (w1, w2, ..., wn),\n",
        "    calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn),\n",
        "    compares them and returns the spam or ham label, respectively.\n",
        "    '''\n",
        "    \n",
        "    # Replace addresses (hhtp, email), numbers (plain, phone), money symbols\n",
        "    message = message.replace(r'\\b[\\w\\‐.]+?@\\w+?\\.\\w{2,4}\\b', ' ')\n",
        "    message = message.replace(r'(http[s]?\\S+)|(\\w+\\.[A‐Za‐z]{2,4}\\S*)', ' ')\n",
        "    message = message.replace(r'£|\\$', ' ')    \n",
        "    message =message.replace(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\‐(.]?\\d{3}\\)?[\\s.‐]?\\d{3}[\\s.‐]?\\d{4}\\b', '')    \n",
        "    message = message.replace(r'\\d+(\\.\\d+)?', ' ')\n",
        "    # Remove punctuation, collapse all whitespace (spaces, line breaks, tabs) into a single space & eliminate any leading/trailing whitespace.\n",
        "    message = message.replace(r'[^\\w\\d\\s]', ' ')\n",
        "    message = message.replace(r'\\s+', ' ')\n",
        "    message = message.replace(r'^\\s+|\\s+?$', '')\n",
        "    # Lowercase the entire corpus\n",
        "    message = message.lower()\n",
        "    \n",
        "    # Remove stop words    \n",
        "    terms = []\n",
        "    for term in message.split():\n",
        "        if term not in set(stop_words):\n",
        "            terms.append(term)\n",
        "            message = ' '.join(terms)\n",
        "    \n",
        "    # Lemmatization\n",
        "    message = ' '.join(lemmatizer.lemmatize(term, pos='v') for term in message.split())\n",
        "    \n",
        "    # Stemming\n",
        "    message = ' '.join(porter.stem(term) for term in message.split())\n",
        "    \n",
        "    # Tokenization\n",
        "    message = message.split()\n",
        "    p_spam_given_message = p_spam\n",
        "    p_ham_given_message = p_ham\n",
        "    for word in message:\n",
        "      if word in parameters_spam:\n",
        "            p_spam_given_message *= parameters_spam[word]\n",
        "      if word in parameters_ham:\n",
        "            p_ham_given_message *= parameters_ham[word]\n",
        "      if p_ham_given_message > p_spam_given_message:\n",
        "            return 'ham'\n",
        "      elif p_spam_given_message > p_ham_given_message:\n",
        "            return 'spam'\n",
        "      else:\n",
        "            return 'needs human classification'\n",
        "test_set['sms_predicted'] = test_set['SMS'].apply(sms_classify_test_set)\n",
        "test_set.head()\n",
        "# Calculate the accuracy of the algorithm\n",
        "correct = 0\n",
        "total = test_set.shape[0]\n",
        "for row in test_set.iterrows():\n",
        "    row = row[1]\n",
        "    if row['Label'] == row['sms_predicted']:\n",
        "        correct += 1\n",
        "print('Results \\n‐‐‐‐‐‐‐')\n",
        "print('Valid:', correct)\n",
        "print('Invalid:', total-correct)\n",
        "print('Accuracy:', round(correct/total, 4))\n"
      ],
      "metadata": {
        "id": "qm0JQIUTotM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZWcHhS9RWDse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}